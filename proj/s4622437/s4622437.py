# -*- coding: utf-8 -*-
"""s4622437.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qdStgOYUg1Q8HCOnyXVWiDE0QW9AexYP
"""

import numpy as np
import pandas as pd
import math
from sklearn import tree
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import tree
import math
from sklearn import metrics, tree, ensemble, neighbors, linear_model
from sklearn.ensemble import VotingClassifier
import csv
# source taken from : https://stackoverflow.com/questions/52346725/can-i-add-outlier-detection-and-removal-to-scikit-learn-pipeline
class OutlierExtractor(TransformerMixin):
  def __init__(self, **kwargs):
    """
    Create a transformer to remove outliers. A threshold is set for selection
    criteria, and further arguments are passed to the LocalOutlierFactor class

    Keyword Args:
        neg_conf_val (float): The threshold for excluding samples with a lower
            negative outlier factor.

    Returns:
        object: to be used as a transformer method as part of Pipeline()
    """

    self.threshold = kwargs.pop('neg_conf_val', -10.0)

    self.kwargs = kwargs

  def transform(self,X, y):
    """
    Uses LocalOutlierFactor class to subselect data based on some threshold

    Returns:
        ndarray: subsampled data

    Notes:
        X should be of shape (n_samples, n_features)
    """
    X = np.asarray(X)
    y = np.asarray(y)
    lcf = LocalOutlierFactor(**self.kwargs)
    lcf.fit(X)

    X_ = pd.DataFrame(X[lcf.negative_outlier_factor_ > self.threshold, :])
    y_ = pd.DataFrame(y[lcf.negative_outlier_factor_ > self.threshold])

    final_df = pd.concat([X_, y_], axis=1, ignore_index=True)

    return final_df

  def fit(self, *args, **kwargs):
    return self


def specific_class_impute(df):
    numerical_0_class_df = (df.loc[df['Target (Column 117)'] == 0]).iloc[:, :103]
    numerical_1_class_df = (df.loc[df['Target (Column 117)'] == 1]).iloc[:, :103]

    nominal_0_class_df = (df.loc[df['Target (Column 117)'] == 0]).iloc[:, 103:116]
    nominal_1_class_df = (df.loc[df['Target (Column 117)'] == 1]).iloc[:, 103:116]
    
    mean_preprocess = SimpleImputer(missing_values=np.nan, strategy='mean')
    mode_preprocess = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

    #mean
    mean_preprocess.fit(numerical_0_class_df)
    numerical_0_class_df_preprocessed = pd.DataFrame(mean_preprocess.transform(numerical_0_class_df), index=numerical_0_class_df.index) 

    mean_preprocess.fit(numerical_1_class_df)
    numerical_1_class_df_preprocessed = pd.DataFrame(mean_preprocess.transform(numerical_1_class_df), index=numerical_1_class_df.index) 

    #concat mean
    mean_final = pd.concat([numerical_0_class_df_preprocessed, numerical_1_class_df_preprocessed], sort=False).sort_index()


    #mode
    mode_preprocess.fit(nominal_0_class_df)
    nominal_0_class_df_preprocessed = pd.DataFrame(mode_preprocess.transform(nominal_0_class_df), index=nominal_0_class_df.index)

    mode_preprocess.fit(nominal_1_class_df)
    nominal_1_class_df_preprocessed = pd.DataFrame(mode_preprocess.transform(nominal_1_class_df), index=nominal_1_class_df.index)
    
    #concat mode
    mode_final = pd.concat([nominal_0_class_df_preprocessed, nominal_1_class_df_preprocessed], sort=False).sort_index()
    
    #concat mode & mean
    final_df = pd.concat([mean_final, mode_final], axis=1, ignore_index=True)

    return final_df

def all_val_impute(X):
    Numerical_df = X.iloc[:, 0:103]
    Nominal_df = X.iloc[:, 103:116]

    mean_preprocess = SimpleImputer(missing_values=np.nan, strategy='mean')
    mode_preprocess = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

    #mean
    mean_preprocess.fit(Numerical_df)
    imputated_with_mean = pd.DataFrame(mean_preprocess.transform(Numerical_df), index=Numerical_df.index) 

    #mode
    mode_preprocess.fit(Nominal_df)
    imputated_with_mode = pd.DataFrame(mode_preprocess.transform(Nominal_df), index=Nominal_df.index) 

    #combining mean and mode treatment
    final_df = pd.concat([imputated_with_mean, imputated_with_mode], axis=1, ignore_index=True)

    return final_df

def main(): 
    df = pd.read_csv(filepath_or_buffer='/content/Ecoli.csv')
    
    df_test = pd.read_csv(filepath_or_buffer='/content/Ecoli_test(1).csv')
    #display(df_test)
    
    """## Preprocessing"""
    
    y = df.iloc[:,116]
    X = df.drop(['Target (Column 117)'], axis=1)
    ##X = data.drop(['Price'], axis=1)
    
    B = all_val_impute(X)
    a = specific_class_impute(df)
 
    all_value_result = all_val_impute(df)
    class_specific_result = specific_class_impute(df)
    
    outlier_process = OutlierExtractor()
    outlier_all_value_result = outlier_process.transform(all_value_result, y)
    outlier_class_specific_result = outlier_process.transform(class_specific_result, y)
    
    
    X_outlier_all_value_result = outlier_all_value_result.iloc[:,0:116]
    y_outlier_all_value_result = outlier_all_value_result.iloc[:,116]
    
    X_outlier_class_specific_result = outlier_class_specific_result.iloc[:,0:116]
    y_outlier_class_specific_result = outlier_class_specific_result.iloc[:,116]
    

    """## Decision Tree"""
    print('============')
    print('DECISION TREE')
    print('============')
    clf = Pipeline(
        steps=[('scaler', MinMaxScaler()) ,("classifier", DecisionTreeClassifier())]
    )
    
    clf2 = Pipeline(
        steps=[('scaler', StandardScaler()) ,("classifier", DecisionTreeClassifier())]
    )
    
    grid_search1 = GridSearchCV(clf, param_grid={'classifier__criterion': ["gini", "entropy"],
             'classifier__max_depth': np.arange(2, 25),
             'classifier__class_weight' : ["balanced"],
             'classifier__random_state' : [5]
             }, cv=10, refit="accuracy", scoring=['accuracy','f1'],verbose=True)
    
    grid_search2 = GridSearchCV(clf2, param_grid={'classifier__criterion': ["gini", "entropy"],
             'classifier__max_depth': np.arange(2, 25),
             'classifier__class_weight' : ["balanced"],
             'classifier__random_state' : [5]
             }, cv=10, refit="accuracy", scoring=['accuracy','f1'],verbose=True)
    
    grid_search3 = GridSearchCV(clf, param_grid={'classifier__criterion': ["gini", "entropy"],
             'classifier__max_depth': np.arange(2, 25),
             'classifier__class_weight' : ["balanced"],
             'classifier__random_state' : [5]
             }, cv=10, refit="accuracy", scoring=['accuracy','f1'],verbose=True)
    
    
    grid_search4 = GridSearchCV(clf2, param_grid={'classifier__criterion': ["gini", "entropy"],
             'classifier__max_depth': np.arange(2, 25),
             'classifier__class_weight' : ["balanced"],
             'classifier__random_state' : [5]
             }, cv=10, refit="accuracy", scoring=['accuracy','f1'],verbose=True)
    print('============')
    grid_search1.fit(X_outlier_class_specific_result,y_outlier_class_specific_result) #class specific, minmax
    print(f"Best of 10 CV Best Param : {grid_search1.best_params_}")
    
    idx = grid_search1.best_index_
    accuracy_score = math.floor(grid_search1.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search1.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    result_test = grid_search1.predict(df_test)
    result_test = result_test.tolist()
    
    #print(result_test)
    
    #with open('s4622437.csv', mode='w', newline='') as resultFile:
          #  fullWriter = csv.writer(resultFile, delimiter=',', quotechar='"', lineterminator=',\r\n', quoting=csv.QUOTE_MINIMAL)
          #  fullWriter.writerows(map(lambda x: [int(x)], result_test))
    
    grid_search2.fit(X_outlier_class_specific_result,y_outlier_class_specific_result) #class specific, standardscaler
    print('============')
    print(f"Best of 10 CV Best Param : {grid_search2.best_params_}")
    
    idx = grid_search2.best_index_
    accuracy_score = math.floor(grid_search2.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search2.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    grid_search3.fit(X_outlier_all_value_result,y_outlier_all_value_result) #all value, min max
    print(f"Best of 10 CV Best Param : {grid_search3.best_params_}")
    
    idx = grid_search3.best_index_
    accuracy_score = math.floor(grid_search3.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search3.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    grid_search4.fit(X_outlier_all_value_result,y_outlier_all_value_result) #all value, standardscaler
    print(f"Best of 10 CV Best Param : {grid_search4.best_params_}")
    
    idx = grid_search4.best_index_
    accuracy_score = math.floor(grid_search4.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search4.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    
    """## Random Forest"""
    print('============')
    print('Random Forest')
    print('============')
    
    clf3 = Pipeline(
        steps=[('scaler', MinMaxScaler()) ,("classifier", RandomForestClassifier())]
    )
    
    clf4 = Pipeline(
        steps=[('scaler', StandardScaler()) ,("classifier", RandomForestClassifier())]
    )
    
    grid_search5 = GridSearchCV(clf3, param_grid={
        "classifier__class_weight" : ["balanced"],
          "classifier__criterion" : ["gini", "entropy"],
          "classifier__max_depth" : np.arange(2, 10, 2),
          "classifier__random_state" : [5]
             }, cv=10, refit="accuracy", scoring=['accuracy','f1'],verbose=True)
    
    grid_search6 = GridSearchCV(clf4, param_grid={"classifier__criterion" : ["gini", "entropy"],
                                                  "classifier__max_depth" : np.arange(2, 10, 2),
                                                  "classifier__class_weight" : ["balanced"] , "classifier__random_state" : [5]
                                                  },
                                 cv=10, refit="accuracy", scoring=['accuracy','f1'],verbose=True)
    
    
    grid_search7 = GridSearchCV(clf3, param_grid={
          "classifier__criterion" : ["gini", "entropy"],
          "classifier__max_depth" : np.arange(2, 10, 2),
          "classifier__class_weight" : ["balanced"],
          "classifier__random_state" : [5]
             }, cv=10,refit="accuracy", scoring=['accuracy','f1'],verbose=True)
    
    grid_search8 = GridSearchCV(clf4, param_grid={"classifier__criterion" : ["gini", "entropy"],
                                                  "classifier__max_depth" : np.arange(2, 10, 2),
                                                  "classifier__class_weight" : ["balanced"] , "classifier__random_state" : [5]
                                                  },
                                 cv=10, refit="accuracy", scoring=['accuracy','f1'],verbose=True)
    
    grid_search5.fit(X_outlier_class_specific_result,y_outlier_class_specific_result) #class specific, minmax
    print('============')
    print(f"Best of 10 CV Best Param : {grid_search5.best_params_}")
    
    idx = grid_search5.best_index_
    accuracy_score = math.floor(grid_search5.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search5.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    grid_search6.fit(X_outlier_class_specific_result,y_outlier_class_specific_result) #class specific, standardscaler
    print(f"Best of 10 CV Best Param : {grid_search6.best_params_}")
    
    idx = grid_search6.best_index_
    accuracy_score = math.floor(grid_search6.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search6.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    grid_search7.fit(X_outlier_all_value_result,y_outlier_all_value_result) #all value, min max
    print(f"Best of 10 CV Best Param : {grid_search7.best_params_}")
    
    idx = grid_search6.best_index_
    accuracy_score = math.floor(grid_search7.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search7.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    grid_search8.fit(X_outlier_all_value_result,y_outlier_all_value_result) #all value, standardscaler
    print(f"Best of 10 CV Best Param : {grid_search8.best_params_}")
    
    idx = grid_search8.best_index_
    accuracy_score = math.floor(grid_search8.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search8.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    
    """## K-Nearest Neighbor"""
    print('============')
    print(' ')
    print('K-NN')
    print('============')
    clf5 = Pipeline(
        steps=[('scaler', MinMaxScaler()), ("classifier", KNeighborsClassifier())]
    )
    
    clf6 = Pipeline(
        steps=[('scaler', StandardScaler()), ("classifier", KNeighborsClassifier())]
    )
    
    
    param_grid = {
        "classifier__n_neighbors" : np.arange(2, 10),
        "classifier__metric" : ["chebyshev","minkowski", "manhattan", "euclidean"]
    }
    
    grid_search5 = GridSearchCV(clf5, param_grid, cv=10, refit="accuracy", scoring=['accuracy','f1'], verbose=True, n_jobs=-1)
    
    grid_search6 = GridSearchCV(clf6, param_grid, cv=10, refit="accuracy", scoring=['accuracy','f1'], verbose=True, n_jobs=-1)
    
    grid_search5.fit(X_outlier_class_specific_result,y_outlier_class_specific_result) #class specific, minmax
    print('============')
    print(f"Best of 10 CV Best Param : {grid_search5.best_params_}")
    idx = grid_search5.best_index_
    accuracy_score = math.floor(grid_search5.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search5.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    grid_search6.fit(X_outlier_all_value_result,y_outlier_all_value_result) #all value, standadrdscaler
    print(f"Best of 10 CV Best Param : {grid_search6.best_params_}")
    
    idx = grid_search6.best_index_
    accuracy_score = math.floor(grid_search6.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search6.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    grid_search5.fit(X_outlier_all_value_result,y_outlier_all_value_result) #allvalues specific, minmax
    print(f"Best of 10 CV Best Param : {grid_search5.best_params_}")
    
    idx = grid_search5.best_index_
    accuracy_score = math.floor(grid_search5.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search5.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    grid_search6.fit(X_outlier_class_specific_result,y_outlier_class_specific_result) #class specific, standadrdscaler
    print(f"Best of 10 CV Best Param : {grid_search6.best_params_}")
    
    idx = grid_search6.best_index_
    accuracy_score = math.floor(grid_search6.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search6.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))

    """## Naive Bayes"""
    print('============')
    print(' ')
    print('Naive Bayes')
    print('============')
    from sklearn.naive_bayes import GaussianNB
    clf = Pipeline(
        steps=[('scaler', MinMaxScaler()), ("classifier", GaussianNB())]
    )
    
    clf2 = Pipeline(
        steps=[('scaler', StandardScaler()), ("classifier", GaussianNB())]
    )
    
    param_grid = { }
    
    grid_search9 = GridSearchCV(clf, param_grid, cv=10, refit="accuracy", scoring=['accuracy','f1'], verbose=True)
    grid_search10 = GridSearchCV(clf2, param_grid, cv=10,refit="accuracy", scoring=['accuracy','f1'], verbose=True)
    print('============')
    grid_search9.fit(X_outlier_all_value_result,y_outlier_all_value_result) #all value, standardscaler
    print(f"Best of 10 CV Best Param : {grid_search9.best_params_}")
    
    idx = grid_search9.best_index_
    accuracy_score = math.floor(grid_search9.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search9.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    grid_search10.fit(X_outlier_class_specific_result,y_outlier_class_specific_result) #specific class, standardscaler
    print(f"Best of 10 CV Best Param : {grid_search10.best_params_}")
    
    idx = grid_search10.best_index_
    accuracy_score = math.floor(grid_search10.best_score_ * 1000)/1000.0
    f1_score = math.floor(grid_search10.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))

    """## Ensemble method"""
    print('============')
    print(' ')
    print('Ensemble method')
    print('============')
    DT = DecisionTreeClassifier(random_state = 5,  class_weight='balanced', criterion = 'entropy', max_depth=3)
    RF = RandomForestClassifier(random_state= 5, class_weight='balanced', criterion = 'entropy', max_depth=4)
    KNN = KNeighborsClassifier(n_neighbors= 3, metric='minkowski')
    
    estimators=[
                ('DecisionTree',DT),
                ('knn', KNN),
                ('Random Forest Entropy',RF)]
    
    clf = Pipeline(
        steps=[('scaler', MinMaxScaler()), ("classifier", VotingClassifier(estimators))]
    )
    
    clf2 = Pipeline(
        steps=[('scaler', StandardScaler()), ("classifier", VotingClassifier(estimators))]
    )
    
    param_grid = {
        "classifier__voting" : ["hard","soft"]
     }
    print('============')
    print('Class Specific Imputation')
    print('MinMaxScaler')
    ###class specific imputation
    votingEnsemble_class_specific = GridSearchCV(clf,param_grid, cv=10, n_jobs=4, refit="accuracy", scoring=['accuracy','f1'])
    votingEnsemble_class_specific.fit(X_outlier_class_specific_result, y_outlier_class_specific_result)
    print(f"Best of 10 CV Best Param : {votingEnsemble_class_specific.best_params_}")
    
    print(f"Best of 10 CV Best Param : {votingEnsemble_class_specific.best_params_}")
    
    idx = votingEnsemble_class_specific.best_index_
    accuracy_score = math.floor(votingEnsemble_class_specific.best_score_ * 1000)/1000.0
    f1_score = math.floor(votingEnsemble_class_specific.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    #######
    print(' ')
    print('Class Specific Imputation')
    print('StandardScaler')
    votingEnsemble_class_specific2 = GridSearchCV(clf2,param_grid, cv=10, n_jobs=4, refit="accuracy", scoring=['accuracy','f1'])
    votingEnsemble_class_specific2.fit(X_outlier_class_specific_result, y_outlier_class_specific_result)
    
    
    print(f"Best of 10 CV Best Param : {votingEnsemble_class_specific2.best_params_}")
    
    idx = votingEnsemble_class_specific2.best_index_
    accuracy_score = math.floor(votingEnsemble_class_specific2.best_score_ * 1000)/1000.0
    f1_score = math.floor(votingEnsemble_class_specific2.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    print('All Value Imputation')
    print('MinMaxScaler')
    ###class specific imputation
    votingEnsemble_class_specific = GridSearchCV(clf,param_grid, cv=10, refit="accuracy", scoring=['accuracy','f1'])
    votingEnsemble_class_specific.fit(X_outlier_all_value_result, y_outlier_all_value_result)
    print(f"Best of 10 CV Best Param : {votingEnsemble_class_specific.best_params_}")
    
    idx = votingEnsemble_class_specific.best_index_
    accuracy_score = math.floor(votingEnsemble_class_specific.best_score_ * 1000)/1000.0
    f1_score = math.floor(votingEnsemble_class_specific.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))
    print('============')
    #######
    print(' ')
    print('All Value Imputation')
    print('StandardScaler')
    votingEnsemble_class_specific2 = GridSearchCV(clf2,param_grid, cv=10,  refit="accuracy", scoring=['accuracy','f1'])
    votingEnsemble_class_specific2.fit(X_outlier_all_value_result, y_outlier_all_value_result)
    
    
    print(f"Best of 10 CV Best Param : {votingEnsemble_class_specific2.best_params_}")
    
    idx = votingEnsemble_class_specific2.best_index_
    accuracy_score = math.floor(votingEnsemble_class_specific2.best_score_ * 1000)/1000.0
    f1_score = math.floor(votingEnsemble_class_specific2.cv_results_['mean_test_f1'][idx] * 1000)/1000.0
    combined_result = (accuracy_score,f1_score)
    
    print("accuracy , F1")
    print(str(combined_result))

if __name__ == "__main__":
    main()